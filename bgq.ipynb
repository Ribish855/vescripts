{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m end_date = \u001b[33m\"\u001b[39m\u001b[33m2024-10-31\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Example end date\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Fetch data from BigQuery\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m data = \u001b[43mread_data_from_bigquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeviceId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Convert the data to a DataFrame\u001b[39;00m\n\u001b[32m     47\u001b[39m df = pd.DataFrame(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mread_data_from_bigquery\u001b[39m\u001b[34m(project_id, dataset_id, table_id, deviceId, start_date, end_date, limit)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_data_from_bigquery\u001b[39m(project_id, dataset_id, table_id, deviceId, start_date, end_date, limit=\u001b[32m10\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Construct a BigQuery client object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     client = \u001b[43mbigquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Define the query to read data with filters for imei and date range\u001b[39;00m\n\u001b[32m     11\u001b[39m     query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33m        SELECT deviceId, hrlfc, totalDistance\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m        FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m        WHERE deviceId = \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeviceId\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m        AND DATE(eDate) BETWEEN \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m AND \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\client.py:253\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, project, credentials, _http, location, default_query_job_config, default_load_job_config, client_info, client_options)\u001b[39m\n\u001b[32m    250\u001b[39m     client_options = google.api_core.client_options.from_dict(client_options)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# assert isinstance(client_options, google.api_core.client_options.ClientOptions)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m kw_args: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mclient_info\u001b[39m\u001b[33m\"\u001b[39m: client_info}\n\u001b[32m    261\u001b[39m bq_host = _get_bigquery_host()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:338\u001b[39m, in \u001b[36mClientWithProject.__init__\u001b[39m\u001b[34m(self, project, credentials, client_options, _http)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=\u001b[38;5;28;01mNone\u001b[39;00m, credentials=\u001b[38;5;28;01mNone\u001b[39;00m, client_options=\u001b[38;5;28;01mNone\u001b[39;00m, _http=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43m_ClientProjectMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     Client.\u001b[34m__init__\u001b[39m(\n\u001b[32m    340\u001b[39m         \u001b[38;5;28mself\u001b[39m, credentials=credentials, client_options=client_options, _http=_http\n\u001b[32m    341\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:286\u001b[39m, in \u001b[36m_ClientProjectMixin.__init__\u001b[39m\u001b[34m(self, project, credentials)\u001b[39m\n\u001b[32m    283\u001b[39m     project = \u001b[38;5;28mgetattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mproject_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     project = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mProject was not passed and could not be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdetermined from the environment.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:305\u001b[39m, in \u001b[36m_ClientProjectMixin._determine_default\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_determine_default\u001b[39m(project):\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper:  use default project detection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_determine_default_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\_helpers\\__init__.py:152\u001b[39m, in \u001b[36m_determine_default_project\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Determine default project ID explicitly or implicitly as fall-back.\u001b[39;00m\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m \u001b[33;03mSee :func:`google.auth.default` for details on how the default project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m:returns: Default project if it can be determined.\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     _, project = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m project\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     credentials, project_id = \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    687\u001b[39m         credentials = with_scopes_if_required(\n\u001b[32m    688\u001b[39m             credentials, scopes, default_scopes=default_scopes\n\u001b[32m    689\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:678\u001b[39m, in \u001b[36mdefault.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcredentials\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CredentialsWithQuotaProject\n\u001b[32m    669\u001b[39m explicit_project_id = os.environ.get(\n\u001b[32m    670\u001b[39m     environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)\n\u001b[32m    671\u001b[39m )\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# safely set on the returned credentials since requires_scopes will\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# guard against setting scopes on user credentials.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43m_get_explicit_environ_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),\n\u001b[32m    680\u001b[39m     _get_gae_credentials,\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m    685\u001b[39m     credentials, project_id = checker()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:293\u001b[39m, in \u001b[36m_get_explicit_environ_credentials\u001b[39m\u001b[34m(quota_project_id)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_gcloud_sdk_credentials(quota_project_id=quota_project_id)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m explicit_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     credentials, project_id = \u001b[43mload_credentials_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[43menvironment_vars\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCREDENTIALS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     credentials._cred_file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplicit_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file via the GOOGLE_APPLICATION_CREDENTIALS environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m credentials, project_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:125\u001b[39m, in \u001b[36mload_credentials_from_file\u001b[39m\u001b[34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads Google credentials from a file.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03mThe credentials file must be a service account key, stored authorized\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m        wrong format or is missing.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(filename):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m was not found.\u001b[39m\u001b[33m\"\u001b[39m.format(filename)\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found."
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, deviceId, start_date, end_date, limit=10):\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define the query to read data with filters for imei and date range\n",
    "    query = f\"\"\"\n",
    "        SELECT deviceId, hrlfc, totalDistance\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{deviceId}' \n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the query and get the result\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "        rows = [dict(row) for row in results]\n",
    "        return rows\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"\n",
    "\n",
    "    # Specify IMEI and date range\n",
    "    deviceId = \"861557068886131\"  # Example IMEI\n",
    "    start_date = \"2024-10-01\"  # Example start date\n",
    "    end_date = \"2024-10-31\"  # Example end date\n",
    "\n",
    "    # Fetch data from BigQuery\n",
    "    data = read_data_from_bigquery(project_id, dataset_id, table_id, deviceId, start_date, end_date)\n",
    "\n",
    "    # Convert the data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m scopes = [\u001b[33m\"\u001b[39m\u001b[33mhttps://www.googleapis.com/auth/cloud-platform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mbigquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconn-datalake-prod\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_latest_packet_from_bigquery\u001b[39m(project_id, dataset_id, table_id, imei, start_date, end_date):\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fetch the latest (most recent) packet for the given IMEI.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\client.py:253\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, project, credentials, _http, location, default_query_job_config, default_load_job_config, client_info, client_options)\u001b[39m\n\u001b[32m    250\u001b[39m     client_options = google.api_core.client_options.from_dict(client_options)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# assert isinstance(client_options, google.api_core.client_options.ClientOptions)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m kw_args: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mclient_info\u001b[39m\u001b[33m\"\u001b[39m: client_info}\n\u001b[32m    261\u001b[39m bq_host = _get_bigquery_host()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:339\u001b[39m, in \u001b[36mClientWithProject.__init__\u001b[39m\u001b[34m(self, project, credentials, client_options, _http)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=\u001b[38;5;28;01mNone\u001b[39;00m, credentials=\u001b[38;5;28;01mNone\u001b[39;00m, client_options=\u001b[38;5;28;01mNone\u001b[39;00m, _http=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    338\u001b[39m     _ClientProjectMixin.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=project, credentials=credentials)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43mClient\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:196\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, credentials, _http, client_options)\u001b[39m\n\u001b[32m    194\u001b[39m         credentials = google.auth.api_key.Credentials(client_options.api_key)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         credentials, _ = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28mself\u001b[39m._credentials = google.auth.credentials.with_scopes_if_required(\n\u001b[32m    199\u001b[39m     credentials, scopes=scopes\n\u001b[32m    200\u001b[39m )\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_options.quota_project_id:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     credentials, project_id = \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    687\u001b[39m         credentials = with_scopes_if_required(\n\u001b[32m    688\u001b[39m             credentials, scopes, default_scopes=default_scopes\n\u001b[32m    689\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:678\u001b[39m, in \u001b[36mdefault.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcredentials\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CredentialsWithQuotaProject\n\u001b[32m    669\u001b[39m explicit_project_id = os.environ.get(\n\u001b[32m    670\u001b[39m     environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)\n\u001b[32m    671\u001b[39m )\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# safely set on the returned credentials since requires_scopes will\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# guard against setting scopes on user credentials.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43m_get_explicit_environ_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),\n\u001b[32m    680\u001b[39m     _get_gae_credentials,\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m    685\u001b[39m     credentials, project_id = checker()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:293\u001b[39m, in \u001b[36m_get_explicit_environ_credentials\u001b[39m\u001b[34m(quota_project_id)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_gcloud_sdk_credentials(quota_project_id=quota_project_id)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m explicit_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     credentials, project_id = \u001b[43mload_credentials_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[43menvironment_vars\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCREDENTIALS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     credentials._cred_file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplicit_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file via the GOOGLE_APPLICATION_CREDENTIALS environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m credentials, project_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:125\u001b[39m, in \u001b[36mload_credentials_from_file\u001b[39m\u001b[34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads Google credentials from a file.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03mThe credentials file must be a service account key, stored authorized\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m        wrong format or is missing.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(filename):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m was not found.\u001b[39m\u001b[33m\"\u001b[39m.format(filename)\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found."
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "def read_latest_packet_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date):\n",
    "    \"\"\"Fetch the latest (most recent) packet for the given IMEI.\"\"\"\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{imei}'\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        rows = [dict(row) for row in results]\n",
    "        return rows\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"An error occurred while fetching data for IMEI {imei}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_excel(data_dict, file_name):\n",
    "    \"\"\"Save data to an Excel file with each IMEI on a separate sheet.\"\"\"\n",
    "    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "        for imei, data in data_dict.items():\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"Saving latest packet for IMEI {imei}: {len(df)} row(s), {len(df.columns)} columns\")\n",
    "            df.to_excel(writer, sheet_name=f\"IMEI_{imei}\", index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"\n",
    "\n",
    "    imei_list = ['359207068291762']\n",
    "    start_date = '2025-04-03'\n",
    "    end_date = '2025-04-03'\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for imei in imei_list:\n",
    "        print(f\"Fetching latest packet for IMEI {imei}...\")\n",
    "        data = read_latest_packet_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date)\n",
    "        data_dict[imei] = data\n",
    "\n",
    "    excel_file_name = \"abishek.xlsx\"\n",
    "    save_to_excel(data_dict, excel_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 27\n",
      "Column names: deviceId, eDateTime, sequenceNumber, longitude, longitude_1, utc, hrlfc, topGear, sweetSpotPercent, seconds, minute, hour, month, day, year, totalDistance, fuelLevel, engineSpeed, engineOperatingHours, vehicleSpeed, engineOilPressure, engineCoolantTemp, accPedalPosition, live, ignitionStatus, sFuelLevelLtrs, sFuelLevelPer\n",
      "Data saved to LocationData.xlsx\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date):\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Create a query string to fetch data for the given IMEIs within the specified date range\n",
    "    imei_str = ', '.join(f\"'{imei}'\" for imei in imei_list)  # Formatting IMEIs for SQL query\n",
    "    query = f\"\"\"\n",
    "        SELECT deviceId,eDateTime,sequenceNumber,longitude,longitude,utc,hrlfc,topGear,sweetSpotPercent,seconds,minute,hour,month,day,year,totalDistance,fuelLevel, engineSpeed, engineOperatingHours, vehicleSpeed, engineOilPressure,\tengineCoolantTemp,accPedalPosition,live,ignitionStatus,sFuelLevelLtrs,sFuelLevelPer\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId IN ({imei_str})\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the query and get the result\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "        rows = [dict(row) for row in results]\n",
    "        return rows\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return []\n",
    "\n",
    "def save_to_excel(data, file_name):\n",
    "    # Convert data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df.to_excel(file_name, index=False)\n",
    "    print(f\"Column names: {', '.join(df.columns)}\")\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "    # List of IMEIs to fetch data for\n",
    "    imei_list = ['352467113127596','352467113210111']\n",
    "    \n",
    "    # Define the date range\n",
    "    start_date = '2024-10-01'\n",
    "    end_date = '2025-03-31'\n",
    "\n",
    "    # Fetch data from BigQuery for specified IMEIs within the date range\n",
    "    data = read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date)\n",
    "    \n",
    "    # Check if data was fetched successfully\n",
    "    if data:\n",
    "        # Define the output Excel file name\n",
    "        excel_file_name = \"LocationData.xlsx\"\n",
    "\n",
    "        # Save the data to an Excel file\n",
    "        save_to_excel(data, excel_file_name)\n",
    "\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for IMEI 359207067650828...\n",
      "Saving data for IMEI 359207067650828: 434 rows, 27 columns\n",
      "Data saved to 359207067650828.xlsx\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date):\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Create a query string to fetch data for the given IMEI within the specified date range\n",
    "    query = f\"\"\"\n",
    "        SELECT deviceId,eDateTime,sequenceNumber,latitude,longitude,utc,hrlfc,topGear,sweetSpotPercent,seconds,minute,hour,month,day,year,totalDistance,fuelLevel, engineSpeed, engineOperatingHours, vehicleSpeed, engineOilPressure,\tengineCoolantTemp,accPedalPosition,live,ignitionStatus,sFuelLevelLtrs,sFuelLevelPer\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{imei}'\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the query and get the result\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "        rows = [dict(row) for row in results]\n",
    "        return rows\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"An error occurred while fetching data for IMEI {imei}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_excel(data_dict, file_name):\n",
    "    # Save the data to an Excel file with multiple sheets\n",
    "    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "        for imei, data in data_dict.items():\n",
    "            # Convert data to a DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"Saving data for IMEI {imei}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            # Save each IMEI's data to a separate sheet\n",
    "            df.to_excel(writer, sheet_name=f\"IMEI_{imei}\", index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "    # List of IMEIs to fetch data for\n",
    "    imei_list = ['352467113273630']\n",
    "    # Define the date range\n",
    "    start_date = '2025-03-01'\n",
    "    end_date = '2025-03-31'\n",
    "\n",
    "    # Dictionary to store data for each IMEI\n",
    "    data_dict = {}\n",
    "\n",
    "    # Fetch data for each IMEI and store it in the dictionary\n",
    "    for imei in imei_list:\n",
    "        print(f\"Fetching data for IMEI {imei}...\")\n",
    "        data = read_data_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date)\n",
    "        data_dict[imei] = data\n",
    "\n",
    "    # Define the output Excel file name\n",
    "    excel_file_name = \"352467113273630.xlsx\"\n",
    "\n",
    "    # Save all data to an Excel file with multiple sheets\n",
    "    save_to_excel(data_dict, excel_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m scopes = [\u001b[33m\"\u001b[39m\u001b[33mhttps://www.googleapis.com/auth/cloud-platform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mbigquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconn-datalake-prod\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_data_from_bigquery\u001b[39m(project_id, dataset_id, table_id, imei, start_date, end_date):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Construct a BigQuery client object\u001b[39;00m\n\u001b[32m     10\u001b[39m     client = bigquery.Client()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\client.py:253\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, project, credentials, _http, location, default_query_job_config, default_load_job_config, client_info, client_options)\u001b[39m\n\u001b[32m    250\u001b[39m     client_options = google.api_core.client_options.from_dict(client_options)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# assert isinstance(client_options, google.api_core.client_options.ClientOptions)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m kw_args: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mclient_info\u001b[39m\u001b[33m\"\u001b[39m: client_info}\n\u001b[32m    261\u001b[39m bq_host = _get_bigquery_host()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:339\u001b[39m, in \u001b[36mClientWithProject.__init__\u001b[39m\u001b[34m(self, project, credentials, client_options, _http)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=\u001b[38;5;28;01mNone\u001b[39;00m, credentials=\u001b[38;5;28;01mNone\u001b[39;00m, client_options=\u001b[38;5;28;01mNone\u001b[39;00m, _http=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    338\u001b[39m     _ClientProjectMixin.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=project, credentials=credentials)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43mClient\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:196\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, credentials, _http, client_options)\u001b[39m\n\u001b[32m    194\u001b[39m         credentials = google.auth.api_key.Credentials(client_options.api_key)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         credentials, _ = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28mself\u001b[39m._credentials = google.auth.credentials.with_scopes_if_required(\n\u001b[32m    199\u001b[39m     credentials, scopes=scopes\n\u001b[32m    200\u001b[39m )\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_options.quota_project_id:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     credentials, project_id = \u001b[43mchecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    687\u001b[39m         credentials = with_scopes_if_required(\n\u001b[32m    688\u001b[39m             credentials, scopes, default_scopes=default_scopes\n\u001b[32m    689\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:678\u001b[39m, in \u001b[36mdefault.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcredentials\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CredentialsWithQuotaProject\n\u001b[32m    669\u001b[39m explicit_project_id = os.environ.get(\n\u001b[32m    670\u001b[39m     environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)\n\u001b[32m    671\u001b[39m )\n\u001b[32m    673\u001b[39m checkers = (\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# Avoid passing scopes here to prevent passing scopes to user credentials.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# with_scopes_if_required() below will ensure scopes/default scopes are\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# safely set on the returned credentials since requires_scopes will\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# guard against setting scopes on user credentials.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43m_get_explicit_environ_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),\n\u001b[32m    680\u001b[39m     _get_gae_credentials,\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: _get_gce_credentials(request, quota_project_id=quota_project_id),\n\u001b[32m    682\u001b[39m )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m checker \u001b[38;5;129;01min\u001b[39;00m checkers:\n\u001b[32m    685\u001b[39m     credentials, project_id = checker()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:293\u001b[39m, in \u001b[36m_get_explicit_environ_credentials\u001b[39m\u001b[34m(quota_project_id)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_gcloud_sdk_credentials(quota_project_id=quota_project_id)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m explicit_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     credentials, project_id = \u001b[43mload_credentials_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[43menvironment_vars\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCREDENTIALS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquota_project_id\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     credentials._cred_file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplicit_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file via the GOOGLE_APPLICATION_CREDENTIALS environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m credentials, project_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:125\u001b[39m, in \u001b[36mload_credentials_from_file\u001b[39m\u001b[34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads Google credentials from a file.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03mThe credentials file must be a service account key, stored authorized\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m        wrong format or is missing.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(filename):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m was not found.\u001b[39m\u001b[33m\"\u001b[39m.format(filename)\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: File C:\\Users\\Ribish\\AppData\\Roaming\\gcloud\\application_default_credentials.json was not found."
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date):\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Create a query string to fetch data for the given IMEI within the specified date range\n",
    "    query = f\"\"\"\n",
    "        SELECT deviceId, eDateTime, engineOperatingHours, totalDistance\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{imei}'\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY eDate DESC\n",
    "        LIMIT 1\n",
    "        ;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the query and get the result\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "        rows = [dict(row) for row in results]\n",
    "        return rows\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"An error occurred while fetching data for IMEI {imei}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_excel(data_dict, file_name):\n",
    "    # Save the data to an Excel file with multiple sheets\n",
    "    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "        for imei, data in data_dict.items():\n",
    "            # Convert data to a DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"Saving data for IMEI {imei}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            # Save each IMEI's data to a separate sheet\n",
    "            df.to_excel(writer, sheet_name=f\"IMEI_{imei}\", index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "    # List of IMEIs to fetch data for\n",
    "    imei_list = ['359207066348721','359207066347285','359207066297837','359207066344894',\n",
    "'359207066493279','359207066429612','359207066347525','359207066078393',\n",
    "'359207066265032','359207066265776','359207066358423','359207066270685',\n",
    "'359207066804640','359207066104397','352467110967788','352467111019530',\n",
    "'359207066286343','359207066324847','359207066355502','359207066114297',\n",
    "'359207066087956','359207066269836','352467111373929','359207067097087',\n",
    "'352467111348988','352467111410747','352467111407107','352467111475351',\n",
    "'352467111444845','352467111086299','352467111423740','352467111399957',\n",
    "'352467111448796','352467111357237','352467111390378','352467111471293',\n",
    "'352467111469552','352467111426834','352467111427998','352467111425869',\n",
    "'352467111373705','352467111438920','352467111390725','359207067110476',\n",
    "'352467111474990','352467111380478','352467111408535','352467111478728',\n",
    "'352467111085572','352467111343518','352467111448192','352467110994600',\n",
    "'352467111400102','359207066940907','352467111406216','352467111426479',\n",
    "'352467111400110','352467111469479','352467111399734','352467111389628',\n",
    "'352467111403874','352467111407966','352467111362872','352467110978447',\n",
    "'352467111408923','352467111376096','352467111388349','352467111372699',\n",
    "'352467111385212','359207067100790','352467111406554','352467111016288',\n",
    "'352467111422304','352467111419508','352467111393919','359207067282184',\n",
    "'359218066030540','359207067823029','359207068174307','359207068124948',\n",
    "'359207068270386','359207068069333','359207068208667','359207068208774',\n",
    "'359207068217411','359207068126737','359207068175239','359207068448776',\n",
    "'359207068508322','359207068385846','359207068448925','359207068475068',\n",
    "'359207068476546','359207068277480','352467112689356','352467112705350',\n",
    "'359207068428018','359207068458486','359207068340916','359207068353265',\n",
    "'359207068347002','359207068368669','359207068361441','359207068368933',\n",
    "'359207068318318','359207068477114','359207068348364','359207068353596',\n",
    "'359207068395506','352467112684647','359207068219391','359207068302874',\n",
    "'359207068416864','359207068414356','359207068510401','359207068461662',\n",
    "'359207068318573','352467112683060','359207068341260','359207068449212',\n",
    "'359207068351889','359207068324720','359207068437696','359207068488541',\n",
    "'359207068431566','359207068522828','359207068524550','359207068457371',\n",
    "'359207068520749','359207068616299','359207068623345','359207068528049',\n",
    "'359207068633823','359207068629920','359207068521101','359207068486503',\n",
    "'359207068617453','352467113106350','352467113135979','352467113123728',\n",
    "'352467113095876','352467113095579','352467113081736','352467113198100',\n",
    "'352467113220680','352467113200096','352467113413822','352467112904276',\n",
    "'352467112953232','352467112828111','352467113020130','352467113409143',\n",
    "'352467113037522','352467113026079','352467113020841','352467112953265',\n",
    "'352467112953331','352467112903112','352467112825117','352467113413954',\n",
    "'352467112953539','352467112953612','352467113020098','352467113343516',\n",
    "'352467113014943','352467113406503','352467113038017','352467113100007',\n",
    "'352467113092477','352467113108364','352467113322627','352467113106228',\n",
    "'352467113107697','352467113123827','352467113106202','352467113406388',\n",
    "'352467113106491','352467113106590','352467113413053','352467113108216',\n",
    "'352467113100577','352467113136449','352467113099886','352467113105733',\n",
    "'352467113099092','352467113108430','352467113107754','352467113224666',\n",
    "'352467113110741','352467113099134','352467113100205','352467113004233',\n",
    "'352467113094937','352467113099423','352467113127661','352467113087022',\n",
    "'352467113004837','352467113091818','352467113100031','352467113101690',\n",
    "'352467113094408','352467113099746','352467113106582','352467113102300',\n",
    "'352467113127091','352467113409986','352467113322783','352467113399278',\n",
    "'352467113313527','352467113399492','352467113319342','352467113322635',\n",
    "'352467113323740','352467113322650','352467113322767','352467113322775',\n",
    "'352467113319383','352467113319151','352467113399534','352467113319326',\n",
    "'352467113313592','352467113313626','352467113324276','352467113323385',\n",
    "'352467113399385','352467113106707','352467113248863','352467113117647',\n",
    "'352467113197300','352467113117704','352467113192061','352467113229889',\n",
    "'352467113195536','352467113295526','352467113200302','352467113328152',\n",
    "'352467113324284','352467113335967','352467113402098','352467113410240',\n",
    "'352467113331099','352467113318229','352467113413509','352467113327576',\n",
    "'352467113222488','352467113200765','352467113204973','352467113148162',\n",
    "'352467113199157','352467113201656','352467113231091','352467113221076',\n",
    "'352467113219195','352467113214998','352467113229871','352467113225614',\n",
    "'352467113225069','352467113195924','352467113212810','352467113215276',\n",
    "'352467113191352','352467113215169','352467113229210','352467113408343',\n",
    "'352467113195882','352467113202001','352467113229640','352467113191451',\n",
    "'352467113229848','352467113215508','352467113089754','352467113312420',\n",
    "'352467113327618','352467113345370','352467113314103','352467113345081',\n",
    "'352467113330018','352467113413905','352467113317270','352467113313808',\n",
    "'352467113413541','352467113346832','352467113343565','352467113227784',\n",
    "'352467113761188','352467113325380','352467113317767','352467113317411',\n",
    "'352467113313121','352467113320126','352467113361583','352467113319532',\n",
    "'352467113320324','352467113361500','352467113348309','352467113313170',\n",
    "'352467113323708','352467113402163','352467113313097','352467113413434',\n",
    "'352467113317387','352467113319623','352467113329978','352467113352798',\n",
    "'352467113317585','352467113317486','352467113813237','352467113748722',\n",
    "'352467113687235','352467113791532','352467113747187','352467113729847',\n",
    "'352467113690577','352467113798594','352467113808062','352467113815091',\n",
    "'352467113808088','352467113722008','352467113761709','352467113780055',\n",
    "'352467113815703','352467113494186','352467113712496','352467113800879',\n",
    "'352467113729540','352467113768977','352467113753045','352467113834977']\n",
    "\n",
    "    # Define the date range\n",
    "    start_date = '2025-04-08'\n",
    "    end_date = '2025-04-08'\n",
    "\n",
    "    # Dictionary to store data for each IMEI\n",
    "    data_dict = {}\n",
    "\n",
    "    # Fetch data for each IMEI and store it in the dictionary\n",
    "    for imei in imei_list:\n",
    "        print(f\"Fetching data for IMEI {imei}...\")\n",
    "        data = read_data_from_bigquery(project_id, dataset_id, table_id, imei, start_date, end_date)\n",
    "        data_dict[imei] = data\n",
    "\n",
    "    # Define the output Excel file name\n",
    "    excel_file_name = \"kccl.xlsx\"\n",
    "\n",
    "    # Save all data to an Excel file with multiple sheets\n",
    "    save_to_excel(data_dict, excel_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Ribish/Downloads/Karan/mysql.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m client = bigquery.Client(project=\u001b[33m'\u001b[39m\u001b[33mconn-datalake-prod\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m CONFIG_FILEPATH = \u001b[33m'\u001b[39m\u001b[33mC:/Users/Ribish/Downloads/Karan/mysql.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_FILEPATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m config_file:\n\u001b[32m     14\u001b[39m     mysql_config = json.load(config_file)\n\u001b[32m     16\u001b[39m HOST = mysql_config[\u001b[33m\"\u001b[39m\u001b[33mHOST\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/Ribish/Downloads/Karan/mysql.json'"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from urllib.parse import quote\n",
    "\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "CONFIG_FILEPATH = r'C:/Users/700023/Documents/mysql.json'\n",
    "with open(CONFIG_FILEPATH) as config_file:\n",
    "    mysql_config = json.load(config_file)\n",
    "\n",
    "HOST = mysql_config[\"HOST\"]\n",
    "PORT = mysql_config[\"PORT\"]\n",
    "USER = mysql_config[\"USER\"]\n",
    "PASSWORD = mysql_config[\"PASSWORD\"]\n",
    "DATABASE = mysql_config[\"DATABASE\"]\n",
    "\n",
    "def connecting_to_mysql(ip_address, port, username, password, database_name):\n",
    "    db_connection_str = f\"mysql+pymysql://{username}:{quote(password)}@{ip_address}:{port}/{database_name}\"\n",
    "    engine = create_engine(db_connection_str)\n",
    "    return engine  # Return engine for queries\n",
    "\n",
    "# Connect to MySQL\n",
    "engine = connecting_to_mysql(HOST, PORT, USER, PASSWORD, DATABASE)\n",
    "\n",
    "def fetch_last_packet_from_table(project_id, dataset_id, table_id, imei, date, columns):\n",
    "    \"\"\"\n",
    "    Fetches the last packet (latest eDateTime) for a given IMEI and date from a specific BigQuery table.\n",
    "    Ensures partition filtering using eDate.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT {', '.join(columns)}\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{imei}' AND eDate = '{date}'\n",
    "        ORDER BY eDateTime DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        data = [dict(row) for row in results]\n",
    "        return data[0] if data else None\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Error fetching data from {table_id} for IMEI {imei}: {e}\")\n",
    "    return None\n",
    "\n",
    "def fetch_last_nonzero_dpf_packet(project_id, dataset_id, table_id, imei, date, columns):\n",
    "    \"\"\"\n",
    "    Fetches the latest packet where particulateTrap1SootLoadPercent is NOT 0.\n",
    "    If all values are 0, returns the most recent packet.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        WITH latest_packets AS (\n",
    "            SELECT {', '.join(columns)}\n",
    "            FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "            WHERE deviceId = '{imei}' AND eDate = '{date}'\n",
    "            ORDER BY eDateTime DESC\n",
    "        )\n",
    "        SELECT * FROM latest_packets\n",
    "        WHERE particulateTrap1SootLoadPercent != 0\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        data = [dict(row) for row in results]\n",
    "        if data:\n",
    "            return data[0]\n",
    "        fallback_query = f\"\"\"\n",
    "            SELECT {', '.join(columns)}\n",
    "            FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "            WHERE deviceId = '{imei}' AND eDate = '{date}'\n",
    "            ORDER BY eDateTime DESC\n",
    "            LIMIT 1\n",
    "        \"\"\"\n",
    "        query_job = client.query(fallback_query)\n",
    "        results = query_job.result()\n",
    "        data = [dict(row) for row in results]\n",
    "        return data[0] if data else None\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Error fetching data from {table_id} for IMEI {imei}: {e}\")\n",
    "    return None\n",
    "\n",
    "def fetch_imei_to_chassis_mapping(engine, imei_list):\n",
    "    \"\"\"Fetches chassis number (vin) mapped to new_device (deviceId) from MySQL.\"\"\"\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "            SELECT vin, new_device \n",
    "            FROM vecvdb.vin_device_mapping \n",
    "            WHERE new_device IN ({', '.join(map(str, imei_list))})\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return dict(zip(df[\"new_device\"], df[\"vin\"]))\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error fetching chassis mapping: {e}\")\n",
    "        return {}\n",
    "def fetch_chassis_and_reg_no_mapping(engine, imei_list):\n",
    "    \"\"\"Fetches chassis number (vin) and registration number (reg_no) mapped to new_device (deviceId) from MySQL.\"\"\"\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "            SELECT vdm.vin AS chassis_no, ucm.reg_no\n",
    "            FROM vecvdb.vin_device_mapping vdm\n",
    "            LEFT JOIN vecvdb.UEL_custid_master ucm ON vdm.vin = ucm.Chassis_no\n",
    "            WHERE vdm.new_device IN ({', '.join(map(str, imei_list))})\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return {row[\"chassis_no\"]: row[\"reg_no\"] for _, row in df.iterrows()}\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error fetching chassis and reg_no mapping: {e}\")\n",
    "        return {}\n",
    "\n",
    "def merge_data(imei, canbs4_data, can2bs6_data, chassis_mapping, reg_no_mapping):\n",
    "    \"\"\"\n",
    "    Merges data from both tables on deviceId and adds registration number.\n",
    "    \"\"\"\n",
    "    if not canbs4_data:\n",
    "        print(f\"No data found in canbs4_pkt for IMEI {imei}\")\n",
    "        return None\n",
    "    if not can2bs6_data:\n",
    "        print(f\"No data found in can2bs6 for IMEI {imei}\")\n",
    "\n",
    "    chassis_no = chassis_mapping.get(imei, \"Unknown\")\n",
    "    reg_no = reg_no_mapping.get(chassis_no, \"Unknown\")\n",
    "\n",
    "    merged_data = {\n",
    "        \"deviceId\": canbs4_data[\"deviceId\"],\n",
    "        \"Chassis_no\": chassis_no,\n",
    "        \"Reg_no\": reg_no,  # Adding Registration Number\n",
    "        \"eDateTime\": canbs4_data[\"eDateTime\"],\n",
    "        \"FuelinLitres\": canbs4_data[\"sfuelLevelltrs\"],\n",
    "        \"DEF Level\": can2bs6_data[\"sFuelLevelLtrs\"] if can2bs6_data else None,\n",
    "        \"DPF\": can2bs6_data[\"particulateTrap1SootLoadPercent\"] if can2bs6_data else None\n",
    "    }\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def save_to_excel(data_list, file_name):\n",
    "    \"\"\"\n",
    "    Saves the final merged data to an Excel file and adds a note for DPF=0.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data_list)\n",
    "    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Last Packet Data', index=False)\n",
    "        workbook = writer.book\n",
    "        sheet = writer.sheets['Last Packet Data']\n",
    "        sheet.cell(row=len(df) + 3, column=1, value=\"Note: If DPF = 0, the vehicle is in stoppage mode.\")\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    imei_list = ['359207067842748', '359207068159761']\n",
    "    date = '2025-02-28'\n",
    "\n",
    "    # Fetch mappings\n",
    "    chassis_mapping = fetch_imei_to_chassis_mapping(engine, imei_list)\n",
    "    reg_no_mapping = fetch_chassis_and_reg_no_mapping(engine, imei_list)\n",
    "\n",
    "    final_data = []\n",
    "    for imei in imei_list:\n",
    "        print(f\"Processing IMEI {imei}...\")\n",
    "        canbs4_data = fetch_last_packet_from_table(project_id, dataset_id, \"canbs4_pkt\", imei, date, [\"deviceId\", \"eDateTime\", \"sfuelLevelltrs\"])\n",
    "        can2bs6_data = fetch_last_nonzero_dpf_packet(project_id, dataset_id, \"can2bs6_pkt\", imei, date, [\"deviceId\", \"sFuelLevelLtrs\", \"particulateTrap1SootLoadPercent\"])\n",
    "        \n",
    "        merged_data = merge_data(imei, canbs4_data, can2bs6_data, chassis_mapping, reg_no_mapping)\n",
    "        \n",
    "        if merged_data:\n",
    "            final_data.append(merged_data)\n",
    "\n",
    "    if final_data:\n",
    "        save_to_excel(final_data, \"l_packet_data.xlsx\")\n",
    "    else:\n",
    "        print(\"No valid data to save.\")\n",
    "\n",
    "        print(\"No valid data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Ribish/Downloads/Karan/mysql.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load MySQL Config\u001b[39;00m\n\u001b[32m     14\u001b[39m CONFIG_FILEPATH = \u001b[33m'\u001b[39m\u001b[33mC:/Users/Ribish/Downloads/Karan/mysql.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_FILEPATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m config_file:\n\u001b[32m     16\u001b[39m     mysql_config = json.load(config_file)\n\u001b[32m     18\u001b[39m HOST = mysql_config[\u001b[33m\"\u001b[39m\u001b[33mHOST\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/Ribish/Downloads/Karan/mysql.json'"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from urllib.parse import quote\n",
    "\n",
    "# BigQuery Client\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "# Load MySQL Config\n",
    "CONFIG_FILEPATH = 'C:/Users/Ribish/Downloads/Karan/mysql.json'\n",
    "with open(CONFIG_FILEPATH) as config_file:\n",
    "    mysql_config = json.load(config_file)\n",
    "\n",
    "HOST = mysql_config[\"HOST\"]\n",
    "PORT = mysql_config[\"PORT\"]\n",
    "USER = mysql_config[\"USER\"]\n",
    "PASSWORD = mysql_config[\"PASSWORD\"]\n",
    "DATABASE = mysql_config[\"DATABASE\"]\n",
    "\n",
    "def connecting_to_mysql(ip_address, port, username, password, database_name):\n",
    "    db_connection_str = f\"mysql+pymysql://{username}:{quote(password)}@{ip_address}:{port}/{database_name}\"\n",
    "    engine = create_engine(db_connection_str)\n",
    "    return engine  # Return engine for queries\n",
    "\n",
    "# Connect to MySQL\n",
    "engine = connecting_to_mysql(HOST, PORT, USER, PASSWORD, DATABASE)\n",
    "\n",
    "def fetch_last_packet_from_table(project_id, dataset_id, table_id, imei, date, columns):\n",
    "    \"\"\"Fetches the last packet (latest eDateTime) for a given IMEI and date from a BigQuery table.\"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT {', '.join(columns)}\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE deviceId = '{imei}' AND eDate = '{date}'  -- Ensuring partition filtering\n",
    "        ORDER BY eDateTime DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        data = [dict(row) for row in results]\n",
    "        return data[0] if data else None  # Return the latest packet or None if no data\n",
    "    except GoogleAPIError as e:\n",
    "        print(f\"Error fetching data from {table_id} for IMEI {imei}: {e}\")\n",
    "    return None  # Return None if an error occurs\n",
    "\n",
    "def fetch_imei_to_chassis_mapping(engine, imei_list):\n",
    "    \"\"\"Fetches chassis number (vin) mapped to new_device (deviceId) from MySQL.\"\"\"\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "            SELECT vin, new_device \n",
    "            FROM vecvdb.vin_device_mapping \n",
    "            WHERE new_device IN ({', '.join(map(str, imei_list))})\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return dict(zip(df[\"new_device\"], df[\"vin\"]))  # Mapping deviceId to chassisNo\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error fetching chassis mapping: {e}\")\n",
    "        return {}\n",
    "\n",
    "def merge_data(imei, canbs4_data, can2bs6_data, chassis_mapping):\n",
    "    \"\"\"Merges data from both tables on deviceId and renames columns as per requirement.\"\"\"\n",
    "    if not canbs4_data:\n",
    "        print(f\"No data found in canbs4_pkt for IMEI {imei}\")\n",
    "        return None\n",
    "    if not can2bs6_data:\n",
    "        print(f\"No data found in can2bs6 for IMEI {imei}\")\n",
    "\n",
    "    merged_data = {\n",
    "        \"deviceId\": canbs4_data[\"deviceId\"],\n",
    "        \"chassisno\": chassis_mapping.get(imei, \"Unknown\"),  # Map chassis number\n",
    "        \"eDateTime\": canbs4_data[\"eDateTime\"],\n",
    "        \"FuelinLitres\": canbs4_data[\"sfuelLevelltrs\"],\n",
    "        \"DEF Level\": can2bs6_data[\"sFuelLevelLtrs\"] if can2bs6_data else None,\n",
    "        \"DPF\": can2bs6_data[\"particulateTrap1SootLoadPercent\"] if can2bs6_data else None\n",
    "    }\n",
    "    return merged_data\n",
    "\n",
    "def save_to_excel(data_list, file_name):\n",
    "    \"\"\"Saves the final merged data to an Excel file and adds a note for DPF=0.\"\"\"\n",
    "    df = pd.DataFrame(data_list)\n",
    "    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Last Packet Data', index=False)\n",
    "\n",
    "        # Adding a note if DPF is 0\n",
    "        workbook = writer.book\n",
    "        sheet = writer.sheets['Last Packet Data']\n",
    "        sheet.cell(row=len(df) + 3, column=1, value=\"Note: If DPF = 0, the vehicle is in stoppage mode.\")\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    imei_list = ['359207067857811', '359207067846152', '359207067861714', '359207067864619']\n",
    "    date = '2025-02-27'  # Ensure this matches eDate partition\n",
    "\n",
    "    # Fetch chassis number mapping\n",
    "    chassis_mapping = fetch_imei_to_chassis_mapping(engine, imei_list)\n",
    "\n",
    "    final_data = []\n",
    "\n",
    "    for imei in imei_list:\n",
    "        print(f\"Processing IMEI {imei}...\")\n",
    "\n",
    "        # Fetch the last packet from both tables\n",
    "        canbs4_columns = [\"deviceId\", \"eDateTime\", \"sfuelLevelltrs\"]\n",
    "        can2bs6_columns = [\"deviceId\", \"sFuelLevelLtrs\", \"particulateTrap1SootLoadPercent\"]\n",
    "\n",
    "        canbs4_data = fetch_last_packet_from_table(project_id, dataset_id, \"canbs4_pkt\", imei, date, canbs4_columns)\n",
    "        can2bs6_data = fetch_last_packet_from_table(project_id, dataset_id, \"can2bs6_pkt\", imei, date, can2bs6_columns)\n",
    "\n",
    "        # Merge and store\n",
    "        merged_data = merge_data(imei, canbs4_data, can2bs6_data, chassis_mapping)\n",
    "        if merged_data:\n",
    "            final_data.append(merged_data)\n",
    "\n",
    "    # Save to Excel\n",
    "    if final_data:\n",
    "        save_to_excel(final_data, \"last1_packet_data.xlsx\")\n",
    "    else:\n",
    "        print(\"No valid data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to vehicle_data_status.xlsx\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "import pandas as pd\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "client = bigquery.Client(project='conn-datalake-prod')\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date):\n",
    "    # Construct a BigQuery client object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    results_summary = []\n",
    "    \n",
    "    for imei in imei_list:\n",
    "        query = f\"\"\"\n",
    "            SELECT COUNT(*) AS record_count\n",
    "            FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "            WHERE deviceId = '{imei}'\n",
    "            AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}';\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Run the query and get the result\n",
    "            query_job = client.query(query)\n",
    "            results = query_job.result()\n",
    "            for row in results:\n",
    "                record_count = row[\"record_count\"]\n",
    "                status = \"Data Found\" if record_count > 0 else \"Data Not Found\"\n",
    "                results_summary.append({\"deviceId\": imei, \"Status\": status})\n",
    "        \n",
    "        except GoogleAPIError as e:\n",
    "            print(f\"An error occurred while processing IMEI {imei}: {e}\")\n",
    "            results_summary.append({\"deviceId\": imei, \"Status\": \"Error\"})\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def save_to_excel(data, file_name):\n",
    "    # Convert data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df.to_excel(file_name, index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "    # List of IMEIs to fetch data for (replace with your list of 600 chassis numbers)\n",
    "    imei_list = ['351510091728017','353701094346059','353701097557231','353701098204965',\n",
    "'353701098217645','353701098219955','357789641760282','357789642806803',\n",
    "'358735076435220','358735077692084','358735078412854','358735078916235',\n",
    "'358735078922993','358735079079272','358735079083308','358735079178041',\n",
    "'358735079180526','358735079200506','358735079208939','358735079211941',\n",
    "'358735079213384','358735079214382','358735079672068','358735079687983',\n",
    "'358735079757315','358735079778659','358735079778832','358735079785589',\n",
    "'358735079785910','358735079795406','862292053229548','862292053230074',\n",
    "'862292053239281','862292053241022','862292053241345','862292053892626',\n",
    "'862292053892717','862292053892725','862292053892949','862292053893004',\n",
    "'862292053893095','862292053895090','862292053895132','862292053895157',\n",
    "'862292053895579','862292053895694','862292053895710','862292053897369',\n",
    "'862292053898250','862292053898664','862292053898698','865784051017187',\n",
    "'865784051030321','865784051042235','865784051330291','865784051340233',\n",
    "'865784051341033','865784051343625','865784051353186','865784054647675',\n",
    "'865784054647865','865784054648145','865784054648236','865784054648293',\n",
    "'865784054648632','865784054648640','865784054648798','865784054649267',\n",
    "'865784054649275','865784054649390','865784054649523','865784054650513',\n",
    "'865784054650786','865784054650927','865784054651081','865784054651412',\n",
    "'865784054651453','865784054651487','865784054651503','865784054651628',\n",
    "'865784054651701','865784054651768','865784054651875','865784054651974',\n",
    "'865784054652014','865784054652071','865784054652147','865784054652303',\n",
    "'865784054896785','865784054896868','865784054896892','865784054896991',\n",
    "'865784054897130','865784054897494','865784054897544','865784054897593',\n",
    "'865784054897676','865784054897734','865784054897890','865784054898062',\n",
    "'865784054898179','865784054898195','865784054898286','865784054898302',\n",
    "'865784054898492','865784054898559','865784054898716','865784054898732',\n",
    "'865784054898906','865784054898914','865784054899078','865784054899201',\n",
    "'865784054899284','865784054899292','865784054899342','865784054899441',\n",
    "'865784054899557','865784054899763','865784054899912','865784054900413',\n",
    "'865784054900629','865784054900835','865784054901270','865784054901296',\n",
    "'865784054901346','865784054901692','865784054902195','865784054902203',\n",
    "'865784054902476','865784054902864','865784054903268','865784054903441',\n",
    "'865784054903573','865784054903599','865784054903672','865784054903771',\n",
    "'865784054904217','865784054904233','865784054904464','865784054904522',\n",
    "'865784054904688','865784054904902','865784054905016','865784054905032',\n",
    "'865784054905073','865784054905685','865784054905834','865784054905867',\n",
    "'865784054906030','865784054906147','865784054906345','865784054906378',\n",
    "'865784054906436','865784055316437','865784055316478','865784055316668',\n",
    "'865784055316890','865784055317161','865784055317468','865784055317518',\n",
    "'865784055317542','865784055317617','865784055317633','865784055317641',\n",
    "'865784055317658','865784055317666','865784055317690','865784055317732',\n",
    "'865784055317740','865784055317757','865784055317781','865784055317799',\n",
    "'865784055317831','865784055317856','865784055317880','865784055317914',\n",
    "'865784055317930','865784055317948','865784055317955','865784055318003',\n",
    "'865784055318060','865784055318078','865784055318169','865784055318318',\n",
    "'865784055318474','865784055318656','865784055318953','865784055318961',\n",
    "'865784055318987','865784055318995','865784055319001','865784055319019',\n",
    "'865784055319035','865784055319050','865784055319100','865784055319167',\n",
    "'865784055319175','865784055319209','865784055319282','865784055319324',\n",
    "'865784055319357','865784055319415','865784055319605','865784055319639',\n",
    "'865784055319704','865784055319712','865784055319720','865784055319928',\n",
    "'865784055319936','865784055319944','865784055319969','865784055320090',\n",
    "'865784055320207','865784055320223','865784055320231','865784055320264',\n",
    "'865784055320272','865784055320280','865784055320348','865784055320397',\n",
    "'865784055320629','865784055320694','865784055320785','865784055320819',\n",
    "'865784055320835','865784055320959','865784055320975','865784055320983',\n",
    "'865784055321015','865784055321056','865784055321080','865784055321155',\n",
    "'865784055321221','865784055321239','865784055321247','865784055321270',\n",
    "'865784055321288','865784055321692','865784055321718','865784055321759',\n",
    "'865784055321775','865784055321866','865784055321874','865784055321999',\n",
    "'865784055322021','865784055322088','865784055322096','865784055322112',\n",
    "'865784055322161','865784055322179','865784055322195','865784055322229',\n",
    "'865784055322252','865784055322294','865784055322310','865784055322401',\n",
    "'865784055322518','865784055322526','865784055322757','865784055322872',\n",
    "'865784055322898','865784055322955','865784055323219','865784055323227',\n",
    "'865784055323318','865784055323367','865784055323391','865784055323409',\n",
    "'865784055323458','865784055323466','865784055323557','865784055323599',\n",
    "'865784055323615','865784055323680','865784055323714','865784055323904',\n",
    "'865784055324019','865784055324076','865784055324308','865784055324324',\n",
    "'865784055324415','865784055324472','865784055324746','865784055324928',\n",
    "'865784055324969','865784055324977','865784055325040','865784055325149',\n",
    "'865784055325263','865784055325305','865784055325313','865784055325339',\n",
    "'865784055325388','865784055325404','865784055325412','865784055325669',\n",
    "'865784055325685','865784055325727','865784055325966','865784055326543',\n",
    "'865784055779501','865784055781267','865784055781507','865784055783297',\n",
    "'865784055786878','865784055797859','865784055797867','865784055797883',\n",
    "'865784055797909','865784055797966','865784055797974','865784055797982',\n",
    "'865784055798022','865784055798063','865784055798071','865784055798089',\n",
    "'865784055798121','865784055798196','865784055798394','865784055798402',\n",
    "'865784055798428','865784055798477','865784055798576','865784055798667',\n",
    "'865784055798766','865784055798782','865784055798808','865784055798816',\n",
    "'865784055799269','865784055799319','865784055799327','865784055799335',\n",
    "'865784055799376','865784055799392','865784055799400','865784055799418',\n",
    "'865784055799426','865784055799434','865784055799442','865784055799459',\n",
    "'865784055799475','865784055799483','865784055799491','865784055799517',\n",
    "'865784055799525','865784055799533','865784055799541','865784055799558',\n",
    "'865784055799574','865784055799582','865784055799590','865784055799608',\n",
    "'865784055799616','865784055799624','865784055799632','865784055799640',\n",
    "'865784055799665','865784055799905','865784055800075','865784055800083',\n",
    "'865784055800109','865784055800133','865784055800141','865784055800158',\n",
    "'865784055800224','865784055800240','865784055800349','865784055800364',\n",
    "'865784055800612','865784055800620','865784055800661','865784055800687',\n",
    "'865784055800778','865784055800802','865784055800851','865784055800893',\n",
    "'865784055800901','865784055800927','865784055800943','865784055800950',\n",
    "'865784055801016','865784055801040','865784055801099','865784055801115',\n",
    "'865784055801123','865784055801149','865784055801172','865784055801420',\n",
    "'865784055801461','865784055801636','865784055801701','865784055801727',\n",
    "'865784055801743','865784055801750','865784055801768','865784055801818',\n",
    "'865784055801834','865784055801883','865784055801891','865784055801909',\n",
    "'865784055802089','865784055802121','865784055802147','865784055802287',\n",
    "'865784055802436','865784055802527','865784055803087','865784055803293',\n",
    "'865784055803848','865784055803863','865784055804077','865784055804192',\n",
    "'865784055804481','865784055804515','865784055804523','865784055804531',\n",
    "'865784055804549','865784055804564','865784055804796','865784055804820',\n",
    "'865784055804838','865784055804846','865784055804853','865784055804887',\n",
    "'865784055805033','865784055805132','865784055805140','865784055805173',\n",
    "'865784055805199','865784055805306','865784055805330','865784055805389',\n",
    "'865784055805439','865784055805447','865784055805694','865784055805801',\n",
    "'865784055805819','865784055805850','865784055805918','865784055806031',\n",
    "'865784055806346','865784055806411','865784055806627','865784055806908',\n",
    "'865784055807914','865784055807948','865784055808482','865784055808755',\n",
    "'865784055808805','865784055808847','865784055808870','865784055808912',\n",
    "'865784055811221','865784055811692','865784055811965','865784055812005',\n",
    "'865784055812179','865784055812229','865784055812245','865784055812294',\n",
    "'865784055813318','865784055815263','865784055815313','865784055815685',\n",
    "'865784055815693','865784055815701','865784055815800','865784055816105',\n",
    "'865784055817632','865784055818481','865784055818499','865784055818507',\n",
    "'865784055818515','865784055818531','865784055818549','865784055818556',\n",
    "'865784055818564','865784055818572','865784055818580','865784055818598',\n",
    "'865784055818606','865784055818622','865784055818630','865784055818655',\n",
    "'865784055818689','865784055818697','865784055818705','865784055818713',\n",
    "'865784055818721','865784055818739','865784055818754','865784055818762',\n",
    "'865784055818770','865784055818788','865784055818796','865784055818804',\n",
    "'865784055818820','865784055818838','865784055818846','865784055818937',\n",
    "'865784055819018','865784055819034','865784055819075','865784055819125',\n",
    "'865784055819141','865784055819182','865784055819281','865784055819448',\n",
    "'865784055819455','865784055819562','865784055819570','865784055819588',\n",
    "'865784055819646','865784055819661','865784055819810','865784055819844',\n",
    "'865784055819851','865784055819893','865784055819927','865784055819935',\n",
    "'865784055819950','865784055819992','865784055820008','865784055820024',\n",
    "'865784055820032','865784055820040','865784055820065','865784055820081',\n",
    "'865784055820099','865784055820115','865784055820123','865784055820149',\n",
    "'865784055820156','865784055820172','865784055820214','865784055820222',\n",
    "'865784055820248','865784055820255','865784055820263','865784055820370',\n",
    "'865784055820396','865784055820412','865784055820511','865784055820586',\n",
    "'865784055820602','865784055820883','865784055820891','865784055820982',\n",
    "'865784055821105','865784055821212','865784055821279','865784055821345',\n",
    "'865784055821360','865784055821386','865784055821394','865784055821493',\n",
    "'865784055821899','865784055822046','865784055822061','865784055822400',\n",
    "'865784055822889','865784055822988','865784055823028','865784055823291',\n",
    "'865784055823887','865784055823952','865784055824182','865784055824190',\n",
    "'865784055824406','865784055824463','865784055824562','865784055824638',\n",
    "'865784055824695','865784055824729','865784055824752','865784055824786',\n",
    "'865784055824810','865784055824828','865784055824851','865784055824893',\n",
    "'865784055824919','865784055825007','865784055825221','865784055825239',\n",
    "'865784055825296','865784055825650','865784055825825','865784055826435',\n",
    "'865784055826625','865784055826708','865784055826765','865784055827243',\n",
    "'865784055827250','865784055827466','865784055827482','865784055827540',\n",
    "'865784055827912','865784055828043','865784055828209','865784055828282',\n",
    "'865784055828316','865784055828407','865784055891710','865784055891751',\n",
    "'865784055891785','865784055891793','865784055891801','865784055891819',\n",
    "'865784055891835','865784055891843','865784055893583','865784055893732',\n",
    "'865784055893765','865784055893781','865784055893815','865784055893823',\n",
    "'865784055893831','865784055893849','865784055893864','865784055894508',\n",
    "'865784055899424','865784055899960','865784055900008','865784055900016',\n",
    "'865784055900024','865784055900032','865784055900040','865784055900115',\n",
    "'865784055900172','865784055900206','865784055900255','865784055900297',\n",
    "'865784055900586','865784055901683','865784055901857','865784055902566',\n",
    "'865784055902574','865784055904422','865784055904489','865784055906641',\n",
    "'865784055906674','865784055906815','865784055906856','865784055906864',\n",
    "'865784055910502','865784055912474','865784055912490','865784055912615',\n",
    "'865784055912623','865784055912979','865784055913183','865784055913563',\n",
    "'865784055913589','865784055913597','865784055913621','865784055913647',\n",
    "'865784055913688','865784055913696','865784055913704','865784055913712',\n",
    "'865784055913720','865784055913738','865784055913746','865784055913753',\n",
    "'865784055913761','865784055913795','865784055920113','865784055920204',\n",
    "'865784055940467','865784055943487','867019050264474','867019050284472']  # Add your actual IMEIs here\n",
    "\n",
    "    # Define the date range\n",
    "    start_date = \"2025-01-20\"\n",
    "    end_date = \"2025-01-20\"\n",
    "\n",
    "    # Fetch data from BigQuery for specified IMEIs within the date range\n",
    "    data = read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date)\n",
    "\n",
    "    # Define the output Excel file name\n",
    "    excel_file_name = \"vehicle_data_status.xlsx\"\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    save_to_excel(data, excel_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packets in GCP but missing in IBM: Empty DataFrame\n",
      "Columns: [deviceId, eDateTime, eDate, packetName, deviceName, errorFlag, sdpArrivalTime, stage1ProcessingTime, stage2ProcessingTime, stage3ProcessingTime, dwIngestionTime, createdDate, updatedDate, schemaVersion, chassisSeries, chassisNumber, triggerType, latitude, longitude, heading, altitude, speed, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelConsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n",
      "Packets in IBM but missing in GCP: Empty DataFrame\n",
      "Columns: [_id, _rev, createdDateTime, createdDate, receivedDateTime, receivedDate, vin, chassisSeries, chassisNumber, triggerType, longitude, latitude, heading, altitude, speed, eventDateTime, eventDate, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelconsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed, moreDataAvailable, requestServerDateTime, requestServerDate, vc_pdatetime, ac_pdatetime, eDateTime, eTime]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 48 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eDateTime_gcp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eDateTime_gcp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(ibm_filtered, gcp_filtered, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ibm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_gcp\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate time difference (delay) in seconds\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meDateTime_gcp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meDateTime_ibm\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[0;32m     34\u001b[0m average_delay \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_diff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage delay between IBM and GCP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_delay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eDateTime_gcp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file paths for IBM and GCP data\n",
    "ibm_file_path = r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx'  # Replace with your actual IBM file path\n",
    "gcp_file_path = r'C:\\Users\\Ribish\\Downloads\\VECV\\fceetdata.xlsx'  # Replace with your actual GCP file path\n",
    "\n",
    "# Load each file as a separate DataFrame\n",
    "ibm_df = pd.read_excel(ibm_file_path)\n",
    "gcp_df = pd.read_excel(gcp_file_path)\n",
    "\n",
    "# Convert 'eDateTime' to datetime format in each DataFrame\n",
    "ibm_df['eDateTime'] = pd.to_datetime(ibm_df['eDateTime'], dayfirst=True)\n",
    "gcp_df['eDateTime'] = pd.to_datetime(gcp_df['eDateTime'], dayfirst=True)\n",
    "\n",
    "# Define the date to analyze\n",
    "date_to_analyze = '2024-10-28'\n",
    "\n",
    "# Filter data for the specified date\n",
    "ibm_filtered = ibm_df[ibm_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "gcp_filtered = gcp_df[gcp_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "\n",
    "# Find packets present in GCP but missing in IBM, and vice versa\n",
    "missing_in_ibm = gcp_filtered[~gcp_filtered['eDateTime'].isin(ibm_filtered['eDateTime'])]\n",
    "missing_in_gcp = ibm_filtered[~ibm_filtered['eDateTime'].isin(gcp_filtered['eDateTime'])]\n",
    "\n",
    "print(\"Packets in GCP but missing in IBM:\", missing_in_ibm)\n",
    "print(\"Packets in IBM but missing in GCP:\", missing_in_gcp)\n",
    "\n",
    "# Merge on 'eDateTime' to identify matching packets\n",
    "merged_df = pd.merge(ibm_filtered, gcp_filtered, on='eDateTime', suffixes=('_ibm', '_gcp'))\n",
    "\n",
    "# Calculate time difference (delay) in seconds\n",
    "merged_df['time_diff'] = (merged_df['eDateTime_gcp'] - merged_df['eDateTime_ibm']).dt.total_seconds()\n",
    "average_delay = merged_df['time_diff'].mean()\n",
    "\n",
    "print(f\"Average delay between IBM and GCP: {average_delay} seconds\")\n",
    "print(\"Time differences (in seconds):\", merged_df['time_diff'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packets in GCP but missing in IBM:\n",
      " Empty DataFrame\n",
      "Columns: [deviceId, eDateTime, eDate, packetName, deviceName, errorFlag, sdpArrivalTime, stage1ProcessingTime, stage2ProcessingTime, stage3ProcessingTime, dwIngestionTime, createdDate, updatedDate, schemaVersion, chassisSeries, chassisNumber, triggerType, latitude, longitude, heading, altitude, speed, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelConsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n",
      "Packets in IBM but missing in GCP:\n",
      " Empty DataFrame\n",
      "Columns: [_id, _rev, createdDateTime, createdDate, receivedDateTime, receivedDate, vin, chassisSeries, chassisNumber, triggerType, longitude, latitude, heading, altitude, speed, eventDateTime, eventDate, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelconsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed, moreDataAvailable, requestServerDateTime, requestServerDate, vc_pdatetime, ac_pdatetime, eDateTime, eTime]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 48 columns]\n",
      "No matching packets to calculate time difference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file paths for IBM and GCP data\n",
    "ibm_file_path = r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx'\n",
    "gcp_file_path = r'C:\\Users\\Ribish\\Downloads\\SortedFileName.xlsx'\n",
    "\n",
    "# Load each file as a separate DataFrame\n",
    "try:\n",
    "    ibm_df = pd.read_excel(ibm_file_path)\n",
    "    gcp_df = pd.read_excel(gcp_file_path)\n",
    "\n",
    "    # Ensure eDateTime column is correctly formatted\n",
    "    ibm_df['eDateTime'] = pd.to_datetime(ibm_df['eDateTime'], errors='coerce', dayfirst=True)\n",
    "    gcp_df['eDateTime'] = pd.to_datetime(gcp_df['eDateTime'], errors='coerce', dayfirst=True)\n",
    "\n",
    "    # Define the date to analyze\n",
    "    date_to_analyze = '2024-10-28'\n",
    "\n",
    "    # Filter data for the specific date\n",
    "    ibm_filtered = ibm_df[ibm_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "    gcp_filtered = gcp_df[gcp_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "\n",
    "    # Identify packets present in GCP but missing in IBM, and vice versa\n",
    "    missing_in_ibm = gcp_filtered[~gcp_filtered['eDateTime'].isin(ibm_filtered['eDateTime'])]\n",
    "    missing_in_gcp = ibm_filtered[~ibm_filtered['eDateTime'].isin(gcp_filtered['eDateTime'])]\n",
    "\n",
    "    print(\"Packets in GCP but missing in IBM:\\n\", missing_in_ibm)\n",
    "    print(\"Packets in IBM but missing in GCP:\\n\", missing_in_gcp)\n",
    "\n",
    "    # Merge on 'eDateTime' to identify matching packets\n",
    "    merged_df = pd.merge(ibm_filtered, gcp_filtered, on='eDateTime', suffixes=('_ibm', '_gcp'))\n",
    "\n",
    "    # Check if time difference calculation is needed\n",
    "    if 'eDateTime_ibm' in merged_df.columns and 'eDateTime_gcp' in merged_df.columns:\n",
    "        merged_df['time_diff'] = (merged_df['eDateTime_gcp'] - merged_df['eDateTime_ibm']).dt.total_seconds()\n",
    "        average_delay = merged_df['time_diff'].mean()\n",
    "        print(f\"Average delay between IBM and GCP: {average_delay} seconds\")\n",
    "        print(\"Time differences (in seconds):\\n\", merged_df['time_diff'].describe())\n",
    "    else:\n",
    "        print(\"No matching packets to calculate time difference.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Column error: {e}. Please verify that both files contain 'eDateTime' column.\")\n",
    "except Exception as ex:\n",
    "    print(f\"An error occurred: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packets in GCP but missing in IBM: Empty DataFrame\n",
      "Columns: [deviceId, eDateTime, eDate, packetName, deviceName, errorFlag, sdpArrivalTime, stage1ProcessingTime, stage2ProcessingTime, stage3ProcessingTime, dwIngestionTime, createdDate, updatedDate, schemaVersion, chassisSeries, chassisNumber, triggerType, latitude, longitude, heading, altitude, speed, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelConsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n",
      "Packets in IBM but missing in GCP: Empty DataFrame\n",
      "Columns: [_id, _rev, createdDateTime, createdDate, receivedDateTime, receivedDate, vin, chassisSeries, chassisNumber, triggerType, longitude, latitude, heading, altitude, speed, eventDateTime, eventDate, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelconsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed, moreDataAvailable, requestServerDateTime, requestServerDate, vc_pdatetime, ac_pdatetime, eDateTime, eTime]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 48 columns]\n",
      "Average delay between IBM and GCP: 0.0 seconds\n",
      "Time differences (in seconds): count    566.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: time_diff, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file paths for IBM and GCP data\n",
    "ibm_file_path = r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx'  # IBM file path\n",
    "gcp_file_path = r'C:\\Users\\Ribish\\Downloads\\SortedFileName.xlsx'  # GCP file path\n",
    "\n",
    "# Load each file as a separate DataFrame\n",
    "ibm_df = pd.read_excel(ibm_file_path)\n",
    "gcp_df = pd.read_excel(gcp_file_path)\n",
    "\n",
    "# Convert 'eDateTime' to datetime format in each DataFrame\n",
    "ibm_df['eDateTime'] = pd.to_datetime(ibm_df['eDateTime'], dayfirst=True)\n",
    "gcp_df['eDateTime'] = pd.to_datetime(gcp_df['eDateTime'], dayfirst=True)\n",
    "\n",
    "# Define the date to analyze\n",
    "date_to_analyze = '2024-10-28'\n",
    "\n",
    "# Filter data for the specified date\n",
    "ibm_filtered = ibm_df[ibm_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "gcp_filtered = gcp_df[gcp_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "\n",
    "# Find packets present in GCP but missing in IBM, and vice versa\n",
    "missing_in_ibm = gcp_filtered[~gcp_filtered['eDateTime'].isin(ibm_filtered['eDateTime'])]\n",
    "missing_in_gcp = ibm_filtered[~ibm_filtered['eDateTime'].isin(gcp_filtered['eDateTime'])]\n",
    "\n",
    "print(\"Packets in GCP but missing in IBM:\", missing_in_ibm)\n",
    "print(\"Packets in IBM but missing in GCP:\", missing_in_gcp)\n",
    "\n",
    "# Merge on 'eDateTime' to identify matching packets\n",
    "merged_df = pd.merge(ibm_filtered, gcp_filtered, on='eDateTime', suffixes=('_ibm', '_gcp'))\n",
    "\n",
    "# Check if there are any matching packets before calculating time difference\n",
    "if not merged_df.empty:\n",
    "    # Calculate time difference (delay) in seconds\n",
    "    merged_df['time_diff'] = (merged_df['eDateTime'] - merged_df['eDateTime']).dt.total_seconds()\n",
    "    average_delay = merged_df['time_diff'].mean()\n",
    "\n",
    "    print(f\"Average delay between IBM and GCP: {average_delay} seconds\")\n",
    "    print(\"Time differences (in seconds):\", merged_df['time_diff'].describe())\n",
    "else:\n",
    "    print(\"No matching packets to calculate time difference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results saved to C:\\Users\\Ribish\\Downloads\\comparison_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file paths for IBM and GCP data\n",
    "ibm_file_path = r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx'  # IBM file path\n",
    "gcp_file_path = r'C:\\Users\\Ribish\\Downloads\\VECV\\fceetdata.xlsx'  # GCP file path\n",
    "\n",
    "# Load each file as a separate DataFrame\n",
    "ibm_df = pd.read_excel(ibm_file_path)\n",
    "gcp_df = pd.read_excel(gcp_file_path)\n",
    "\n",
    "# Convert 'eDateTime' to datetime format in each DataFrame\n",
    "ibm_df['eDateTime'] = pd.to_datetime(ibm_df['eDateTime'], dayfirst=True)\n",
    "gcp_df['eDateTime'] = pd.to_datetime(gcp_df['eDateTime'], dayfirst=True)\n",
    "\n",
    "# Define the date to analyze\n",
    "date_to_analyze = '2024-10-28'\n",
    "\n",
    "# Filter data for the specified date\n",
    "ibm_filtered = ibm_df[ibm_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "gcp_filtered = gcp_df[gcp_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "\n",
    "# Find packets present in GCP but missing in IBM, and vice versa\n",
    "missing_in_ibm = gcp_filtered[~gcp_filtered['eDateTime'].isin(ibm_filtered['eDateTime'])]\n",
    "missing_in_gcp = ibm_filtered[~ibm_filtered['eDateTime'].isin(gcp_filtered['eDateTime'])]\n",
    "\n",
    "# Merge on 'eDateTime' to identify matching packets\n",
    "merged_df = pd.merge(ibm_filtered, gcp_filtered, on='eDateTime', suffixes=('_ibm', '_gcp'))\n",
    "\n",
    "# Check if there are any matching packets before calculating time difference\n",
    "if not merged_df.empty:\n",
    "    # Calculate time difference (delay) in seconds\n",
    "    merged_df['time_diff'] = (merged_df['eDateTime'] - merged_df['eDateTime']).dt.total_seconds()\n",
    "    average_delay = merged_df['time_diff'].mean()\n",
    "    merged_df['average_delay'] = average_delay  # Add average delay as a new column\n",
    "else:\n",
    "    merged_df['time_diff'] = None\n",
    "    print(\"No matching packets to calculate time difference.\")\n",
    "\n",
    "# Save the data to an Excel file with multiple sheets\n",
    "output_file_path = r'C:\\Users\\Ribish\\Downloads\\comparison_results.xlsx'\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    missing_in_ibm.to_excel(writer, sheet_name='Missing in IBM', index=False)\n",
    "    missing_in_gcp.to_excel(writer, sheet_name='Missing in GCP', index=False)\n",
    "    merged_df.to_excel(writer, sheet_name='Matching Packets', index=False)\n",
    "\n",
    "print(f\"Comparison results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheets merged and saved to 'merged_output.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two Excel sheets\n",
    "sheet1 = pd.read_excel(r'C:\\Users\\700023\\Downloads\\359207068358025bs6.xlsx')\n",
    "sheet2 = pd.read_excel(r'C:\\Users\\700023\\Downloads\\359207068358025.xlsx')\n",
    "\n",
    "# Merge both sheets based on the 'eDateTime' column\n",
    "merged_sheet = pd.merge(sheet1, sheet2, on='eDateTime', how='inner')\n",
    "\n",
    "# Save the merged result to a new Excel file\n",
    "merged_sheet.to_excel('merged1_output.xlsx', index=False)\n",
    "\n",
    "print(\"Sheets merged and saved to 'merged_output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheets merged and saved to 'merged1_output.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file1_path = r'C:\\Users\\700023\\Downloads\\Probs4.xlsx'\n",
    "file2_path = r'C:\\Users\\700023\\Downloads\\Probs6.xlsx'\n",
    "\n",
    "# Load all sheets from both Excel files\n",
    "sheets1 = pd.read_excel(file1_path, sheet_name=None)\n",
    "sheets2 = pd.read_excel(file2_path, sheet_name=None)\n",
    "\n",
    "# Create an empty list to store merged DataFrames\n",
    "merged_sheets = []\n",
    "\n",
    "# Iterate through each sheet\n",
    "for sheet_name in sheets1.keys():\n",
    "    if sheet_name in sheets2:\n",
    "        # Read corresponding sheets\n",
    "        df1 = sheets1[sheet_name]\n",
    "        df2 = sheets2[sheet_name]\n",
    "\n",
    "        # Merge the sheets on 'deviceId' and 'eDateTime'\n",
    "        merged_df = pd.merge(df1, df2, on=['deviceId', 'eDateTime'], how='inner')\n",
    "\n",
    "        # Add merged DataFrame to the list\n",
    "        merged_sheets.append((sheet_name, merged_df))\n",
    "\n",
    "# Save merged sheets to a new Excel file\n",
    "with pd.ExcelWriter('merged1_output.xlsx') as writer:\n",
    "    for sheet_name, merged_df in merged_sheets:\n",
    "        merged_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Sheets merged and saved to 'merged1_output.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sorted by 'eDateTime' and saved to C:\\Users\\Ribish\\Downloads\\23l.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the path to your Excel file\n",
    "file_path = r'C:\\Users\\Ribish\\Downloads\\VECV\\CVdata.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert 'eDateTime' column to datetime format if it's not already\n",
    "df['eDateTime'] = pd.to_datetime(df['eDateTime'], errors='coerce')\n",
    "\n",
    "# Sort the DataFrame by 'eDateTime' in ascending order\n",
    "df_sorted = df.sort_values(by='eDateTime').reset_index(drop=True)\n",
    "\n",
    "# Save the sorted DataFrame back to Excel\n",
    "sorted_file_path = r'C:\\Users\\Ribish\\Downloads\\23l.xlsx'\n",
    "df_sorted.to_excel(sorted_file_path, index=False)\n",
    "\n",
    "print(f\"Data sorted by 'eDateTime' and saved to {sorted_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['AW'] (sheet: 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:607\u001b[0m, in \u001b[0;36mPythonParser._handle_usecols\u001b[1;34m(self, columns, usecols_key, num_original_columns)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     col_indices\u001b[38;5;241m.\u001b[39mappend(\u001b[43musecols_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: 'AW' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m file2_path \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRibish\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m23R.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Read both files, setting `AW` as `eDate` in the first file and `B` as `eDate` in the second\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m...other_columns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m...other_columns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(file2_path, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...other_columns\u001b[39m\u001b[38;5;124m'\u001b[39m], names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...other_columns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Continue with merging and processing, assuming both now have 'eDate' as the column name\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Perform the merge on 'eDate' (or 'eDateTime' if you have timestamp details in this column)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:916\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    915\u001b[0m         err\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (sheet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39merr\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m--> 916\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_sheetname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSheet name is an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:878\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# GH 12292 : error when read one empty column from excel file\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextParser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_index_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_index_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_blank_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GH 39808\u001b[39;49;00m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    903\u001b[0m     output[asheetname] \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mread(nrows\u001b[38;5;241m=\u001b[39mnrows)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m header_names:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:2053\u001b[0m, in \u001b[0;36mTextParser\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;124;03mConverts lists of lists/tuples into DataFrames with proper type inference\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;124;03mand optional (e.g. string to datetime) conversion. Also enables iterating\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;124;03m    `round_trip` for the round-trip converter.\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2052\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:133\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[0;32m    129\u001b[0m (\n\u001b[0;32m    130\u001b[0m     columns,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[1;32m--> 133\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:532\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass names with multi-index columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;66;03m# Set _use_cols. We don't store columns because they are\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# overwritten.\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_usecols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_original_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     num_original_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(names)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:609\u001b[0m, in \u001b[0;36mPythonParser._handle_usecols\u001b[1;34m(self, columns, usecols_key, num_original_columns)\u001b[0m\n\u001b[0;32m    607\u001b[0m         col_indices\u001b[38;5;241m.\u001b[39mappend(usecols_key\u001b[38;5;241m.\u001b[39mindex(col))\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     col_indices\u001b[38;5;241m.\u001b[39mappend(col)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\base_parser.py:979\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    977\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    982\u001b[0m     )\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['AW'] (sheet: 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the two Excel files\n",
    "file1_path = (r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2M6GRC0LC064500.xlsx')\n",
    "file2_path = (r'C:\\Users\\Ribish\\Downloads\\23R.xlsx')\n",
    "\n",
    "\n",
    "# Read both files, setting `AW` as `eDate` in the first file and `B` as `eDate` in the second\n",
    "df1 = pd.read_excel(file1_path, usecols=['AW', '...other_columns'], names=['eDate', '...other_columns'])\n",
    "df2 = pd.read_excel(file2_path, usecols=['B', '...other_columns'], names=['eDate', '...other_columns'])\n",
    "\n",
    "# Continue with merging and processing, assuming both now have 'eDate' as the column name\n",
    "# Perform the merge on 'eDate' (or 'eDateTime' if you have timestamp details in this column)\n",
    "merged_df = pd.merge(df1, df2, on='eDate', suffixes=('_file1', '_file2'))\n",
    "\n",
    "# Calculate time difference or any other analysis you need\n",
    "if 'eDateTime_file1' in merged_df.columns and 'eDateTime_file2' in merged_df.columns:\n",
    "    merged_df['time_diff'] = (merged_df['eDateTime_file1'] - merged_df['eDateTime_file2']).dt.total_seconds()\n",
    "    average_delay = merged_df['time_diff'].mean()\n",
    "    print(\"Average delay:\", average_delay)\n",
    "else:\n",
    "    print(\"eDateTime columns missing in one of the files.\")\n",
    "\n",
    "# Save the merged DataFrame if needed\n",
    "output_path = 'C:/Users/Ribish/Downloads/merged_comparison.xlsx'\n",
    "merged_df.to_excel(output_path, index=False)\n",
    "print(f\"Merged data saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packets in GCP but missing in IBM: Empty DataFrame\n",
      "Columns: [deviceId, eDateTime, eDate, packetName, deviceName, errorFlag, sdpArrivalTime, stage1ProcessingTime, stage2ProcessingTime, stage3ProcessingTime, dwIngestionTime, createdDate, updatedDate, schemaVersion, chassisSeries, chassisNumber, triggerType, latitude, longitude, heading, altitude, speed, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelConsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 46 columns]\n",
      "Packets in IBM but missing in GCP: Empty DataFrame\n",
      "Columns: [_id, _rev, createdDateTime, createdDate, receivedDateTime, receivedDate, vin, chassisSeries, chassisNumber, triggerType, longitude, latitude, heading, altitude, speed, eventDateTime, eventDate, totalDistance, totalEngineHours, totalFuelConsumption, brakeCount, coastingDistance, coastingTime, cruiseDistance, cruiseFuelConsumption, cruiseTime, fuelLevel, idleFuelConsumption, idleTime, movingFuelConsumption, movingTime, ptoFuelconsumption, ptoTime, stopCount, sweetSpotDistance, sweetSpotFuelConsumption, sweetSpotTime, topGearFuelConsumption, topGearTime, topGearDistance, totalAdBlueUsed, moreDataAvailable, requestServerDateTime, requestServerDate, vc_pdatetime, ac_pdatetime, eDateTime, eTime]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 48 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eDateTime_gcp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eDateTime_gcp'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[31], line 33\u001b[0m\n",
      "\u001b[0;32m     30\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(ibm_filtered, gcp_filtered, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ibm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_gcp\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate time difference (delay) in seconds\u001b[39;00m\n",
      "\u001b[1;32m---> 33\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meDateTime_gcp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meDateTime_ibm\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "\u001b[0;32m     34\u001b[0m average_delay \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_diff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage delay between IBM and GCP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_delay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n",
      "\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n",
      "\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n",
      "\u001b[0;32m   3810\u001b[0m     ):\n",
      "\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n",
      "\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n",
      "\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eDateTime_gcp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file paths for IBM and GCP data\n",
    "ibm_file_path = r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx'  # Replace with your actual IBM file path\n",
    "gcp_file_path = r'C:\\Users\\Ribish\\Downloads\\VECV\\fceetdata.xlsx'  # Replace with your actual GCP file path\n",
    "\n",
    "# Load each file as a separate DataFrame\n",
    "ibm_df = pd.read_excel(ibm_file_path)\n",
    "gcp_df = pd.read_excel(gcp_file_path)\n",
    "\n",
    "# Convert 'eDateTime' to datetime format in each DataFrame\n",
    "ibm_df['eDateTime'] = pd.to_datetime(ibm_df['eDateTime'], dayfirst=True)\n",
    "gcp_df['eDateTime'] = pd.to_datetime(gcp_df['eDateTime'], dayfirst=True)\n",
    "\n",
    "# Define the date to analyze\n",
    "date_to_analyze = '2024-10-28'\n",
    "\n",
    "# Filter data for the specified date\n",
    "ibm_filtered = ibm_df[ibm_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "gcp_filtered = gcp_df[gcp_df['eDateTime'].dt.date == pd.to_datetime(date_to_analyze).date()]\n",
    "\n",
    "# Find packets present in GCP but missing in IBM, and vice versa\n",
    "missing_in_ibm = gcp_filtered[~gcp_filtered['eDateTime'].isin(ibm_filtered['eDateTime'])]\n",
    "missing_in_gcp = ibm_filtered[~ibm_filtered['eDateTime'].isin(gcp_filtered['eDateTime'])]\n",
    "\n",
    "print(\"Packets in GCP but missing in IBM:\", missing_in_ibm)\n",
    "print(\"Packets in IBM but missing in GCP:\", missing_in_gcp)\n",
    "\n",
    "# Merge on 'eDateTime' to identify matching packets\n",
    "merged_df = pd.merge(ibm_filtered, gcp_filtered, on='eDateTime', suffixes=('_ibm', '_gcp'))\n",
    "\n",
    "# Calculate time difference (delay) in seconds\n",
    "merged_df['time_diff'] = (merged_df['eDateTime_gcp'] - merged_df['eDateTime_ibm']).dt.total_seconds()\n",
    "average_delay = merged_df['time_diff'].mean()\n",
    "\n",
    "print(f\"Average delay between IBM and GCP: {average_delay} seconds\")\n",
    "print(\"Time differences (in seconds):\", merged_df['time_diff'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to 'analysis_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel sheets\n",
    "excel1 = pd.read_excel(r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx')\n",
    "excel2 = pd.read_excel( r'C:\\Users\\Ribish\\Downloads\\VECV\\fceetdata.xlsx')\n",
    "\n",
    "# Standardize `eDateTime` format\n",
    "excel1['eDateTime'] = pd.to_datetime(excel1['eDateTime'])\n",
    "excel2['eDateTime'] = pd.to_datetime(excel2['eDateTime'])\n",
    "\n",
    "# Check for missing entries by comparing `eDateTime`\n",
    "missing_in_excel2 = excel1[~excel1['eDateTime'].isin(excel2['eDateTime'])]\n",
    "missing_in_excel1 = excel2[~excel2['eDateTime'].isin(excel1['eDateTime'])]\n",
    "\n",
    "# Generate summary statistics\n",
    "summary = {\n",
    "    \"Total in Excel1\": [len(excel1)],\n",
    "    \"Total in Excel2\": [len(excel2)],\n",
    "    \"Missing in Excel2\": [len(missing_in_excel2)],\n",
    "    \"Missing in Excel1\": [len(missing_in_excel1)],\n",
    "    \"Duplicate entries in Excel1\": [excel1['eDateTime'].duplicated().sum()],\n",
    "    \"Duplicate entries in Excel2\": [excel2['eDateTime'].duplicated().sum()],\n",
    "}\n",
    "\n",
    "# Convert summary dictionary to DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Save the results to a new Excel file with multiple sheets\n",
    "with pd.ExcelWriter(\"analysis_results.xlsx\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    missing_in_excel2.to_excel(writer, sheet_name=\"Missing in Excel2\", index=False)\n",
    "    missing_in_excel1.to_excel(writer, sheet_name=\"Missing in Excel1\", index=False)\n",
    "\n",
    "print(\"Analysis complete. Results saved to 'analysis_results.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to 'analysis_results_with_duplicates.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel sheets\n",
    "excel1 = pd.read_excel(r'C:\\Users\\Ribish\\Downloads\\Karan\\Karan\\FUEL_MC2BMLRC0MF072783.xlsx')\n",
    "excel2 = pd.read_excel( r'C:\\Users\\Ribish\\Downloads\\VECV\\fceetdata.xlsx')\n",
    "\n",
    "# Standardize `eDateTime` format\n",
    "excel1['eDateTime'] = pd.to_datetime(excel1['eDateTime'])\n",
    "excel2['eDateTime'] = pd.to_datetime(excel2['eDateTime'])\n",
    "\n",
    "# Check for missing entries by comparing `eDateTime`\n",
    "missing_in_excel2 = excel1[~excel1['eDateTime'].isin(excel2['eDateTime'])]\n",
    "missing_in_excel1 = excel2[~excel2['eDateTime'].isin(excel1['eDateTime'])]\n",
    "\n",
    "# Find duplicate entries in each sheet based on `eDateTime`\n",
    "duplicates_in_excel1 = excel1[excel1.duplicated(subset=['eDateTime'], keep=False)]\n",
    "duplicates_in_excel2 = excel2[excel2.duplicated(subset=['eDateTime'], keep=False)]\n",
    "\n",
    "# Generate summary statistics\n",
    "summary = {\n",
    "    \"Total in Excel1\": [len(excel1)],\n",
    "    \"Total in Excel2\": [len(excel2)],\n",
    "    \"Missing in Excel2\": [len(missing_in_excel2)],\n",
    "    \"Missing in Excel1\": [len(missing_in_excel1)],\n",
    "    \"Duplicate entries in Excel1\": [len(duplicates_in_excel1)],\n",
    "    \"Duplicate entries in Excel2\": [len(duplicates_in_excel2)],\n",
    "}\n",
    "\n",
    "# Convert summary dictionary to DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Save the results to a new Excel file with multiple sheets\n",
    "with pd.ExcelWriter(\"analysis_results_with_duplicates.xlsx\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    missing_in_excel2.to_excel(writer, sheet_name=\"Missing in Excel2\", index=False)\n",
    "    missing_in_excel1.to_excel(writer, sheet_name=\"Missing in Excel1\", index=False)\n",
    "    duplicates_in_excel1.to_excel(writer, sheet_name=\"Duplicates in Excel1\", index=False)\n",
    "    duplicates_in_excel2.to_excel(writer, sheet_name=\"Duplicates in Excel2\", index=False)\n",
    "\n",
    "print(\"Analysis complete. Results saved to 'analysis_results_with_duplicates.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates highlighted and saved to 'highlighted_duplicates.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# Load the Excel sheet\n",
    "file_path = r\"C:\\Users\\Ribish\\Downloads\\SFileName.xlsx\"\n",
    "\n",
    "# Identify duplicates in the `eDateTime` column\n",
    "df['is_duplicate'] = df.duplicated(subset=['eDateTime'], keep=False)\n",
    "\n",
    "# Save the modified DataFrame back to the Excel file (temporarily to work with openpyxl)\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "# Load the workbook with openpyxl to apply formatting\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Define colors for highlighting\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")  # Green for unique\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")    # Red for duplicates\n",
    "\n",
    "# Get the column index for `eDateTime`\n",
    "eDateTime_col = df.columns.get_loc(\"eDateTime\") + 1  # +1 because openpyxl is 1-indexed\n",
    "\n",
    "# Apply color based on duplicate status\n",
    "for row in range(2, len(df) + 2):  # Start from row 2 to skip the header\n",
    "    cell = ws.cell(row=row, column=eDateTime_col)\n",
    "    if df.at[row - 2, 'is_duplicate']:  # Row - 2 to match DataFrame indexing\n",
    "        cell.fill = red_fill\n",
    "    else:\n",
    "        cell.fill = green_fill\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"highlighted_duplicates.xlsx\")\n",
    "wb.close()\n",
    "\n",
    "print(\"Duplicates highlighted and saved to 'highlighted_duplicates.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates highlighted and saved to 'highlighted_duplicates_first_occurrence.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = r\"C:\\Users\\Ribish\\Downloads\\SFileName.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Identify the first occurrence and repetitions in the `eDateTime` column\n",
    "df['is_first_occurrence'] = ~df.duplicated(subset=['eDateTime'], keep='first')\n",
    "df['is_duplicate'] = df.duplicated(subset=['eDateTime'], keep=False)\n",
    "\n",
    "# Save the modified DataFrame temporarily back to the Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "# Load the workbook with openpyxl to apply formatting\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Define colors for highlighting\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")  # Green for first occurrences\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")    # Red for subsequent duplicates\n",
    "\n",
    "# Get the column index for `eDateTime`\n",
    "eDateTime_col = df.columns.get_loc(\"eDateTime\") + 1  # +1 because openpyxl is 1-indexed\n",
    "\n",
    "# Apply color based on first occurrence and duplicates\n",
    "for row in range(2, len(df) + 2):  # Start from row 2 to skip the header\n",
    "    cell = ws.cell(row=row, column=eDateTime_col)\n",
    "    if df.at[row - 2, 'is_first_occurrence']:\n",
    "        cell.fill = green_fill\n",
    "    elif df.at[row - 2, 'is_duplicate']:\n",
    "        cell.fill = red_fill\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"highlighted_duplicates_first_occurrence.xlsx\")\n",
    "wb.close()\n",
    "\n",
    "print(\"Duplicates highlighted and saved to 'highlighted_duplicates_first_occurrence.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates highlighted and summary saved to 'highlighted_duplicates_with_summary.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.chart import PieChart, Reference\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = r\"C:\\Users\\Ribish\\Downloads\\SFileName.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Identify first occurrence and repetitions in the `eDateTime` column\n",
    "df['is_first_occurrence'] = ~df.duplicated(subset=['eDateTime'], keep='first')\n",
    "df['is_duplicate'] = df.duplicated(subset=['eDateTime'], keep=False)\n",
    "\n",
    "# Count green and red markings\n",
    "green_count = df['is_first_occurrence'].sum()\n",
    "red_count = df['is_duplicate'].sum() - green_count  # Total duplicates minus first occurrences\n",
    "\n",
    "# Save the modified DataFrame temporarily back to the Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "# Load the workbook with openpyxl to apply formatting\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Define colors for highlighting\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")  # Green for first occurrences\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")    # Red for subsequent duplicates\n",
    "\n",
    "# Get the column index for `eDateTime`\n",
    "eDateTime_col = df.columns.get_loc(\"eDateTime\") + 1  # +1 because openpyxl is 1-indexed\n",
    "\n",
    "# Apply color based on first occurrence and duplicates\n",
    "for row in range(2, len(df) + 2):  # Start from row 2 to skip the header\n",
    "    cell = ws.cell(row=row, column=eDateTime_col)\n",
    "    if df.at[row - 2, 'is_first_occurrence']:\n",
    "        cell.fill = green_fill\n",
    "    elif df.at[row - 2, 'is_duplicate']:\n",
    "        cell.fill = red_fill\n",
    "\n",
    "# Add a summary table for green and red counts\n",
    "summary_row = len(df) + 4  # Place it a few rows after the data\n",
    "ws.cell(row=summary_row, column=1, value=\"Color\")\n",
    "ws.cell(row=summary_row, column=2, value=\"Count\")\n",
    "ws.cell(row=summary_row + 1, column=1, value=\"Green (First Occurrences)\")\n",
    "ws.cell(row=summary_row + 1, column=2, value=green_count)\n",
    "ws.cell(row=summary_row + 2, column=1, value=\"Red (Subsequent Duplicates)\")\n",
    "ws.cell(row=summary_row + 2, column=2, value=red_count)\n",
    "\n",
    "# Create a pie chart for the green and red counts\n",
    "pie = PieChart()\n",
    "labels = Reference(ws, min_col=1, min_row=summary_row + 1, max_row=summary_row + 2)\n",
    "data = Reference(ws, min_col=2, min_row=summary_row, max_row=summary_row + 2)\n",
    "pie.add_data(data, titles_from_data=True)\n",
    "pie.set_categories(labels)\n",
    "pie.title = \"eDateTime Duplicate Analysis\"\n",
    "\n",
    "# Place the pie chart in the Excel sheet\n",
    "ws.add_chart(pie, f\"D{summary_row}\")\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"highlighted_duplicates_with_summary.xlsx\")\n",
    "wb.close()\n",
    "\n",
    "print(\"Duplicates highlighted and summary saved to 'highlighted_duplicates_with_summary.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in counts.\n",
      "Duplicates highlighted and summary saved to 'highlighted_duplicates_with_summary_corrected.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.chart import PieChart, Reference\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = r\"C:\\Users\\Ribish\\Downloads\\SFileName.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Mark first occurrence and subsequent duplicates\n",
    "df['is_first_occurrence'] = ~df.duplicated(subset=['eDateTime'], keep='first')\n",
    "df['is_duplicate'] = df.duplicated(subset=['eDateTime'], keep=False)\n",
    "\n",
    "# Count green and red markings\n",
    "green_count = df['is_first_occurrence'].sum()  # First occurrences including unique entries\n",
    "red_count = df['is_duplicate'].sum() - green_count  # Total duplicates minus first occurrences\n",
    "\n",
    "# Verify total\n",
    "if green_count + red_count == len(df):\n",
    "    print(\"Counts verified correctly.\")\n",
    "else:\n",
    "    print(\"Mismatch in counts.\")\n",
    "\n",
    "# Load the workbook with openpyxl to apply formatting\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Define colors for highlighting\n",
    "green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")  # Green for first occurrences\n",
    "red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")    # Red for subsequent duplicates\n",
    "\n",
    "# Get the column index for `eDateTime`\n",
    "eDateTime_col = df.columns.get_loc(\"eDateTime\") + 1  # +1 because openpyxl is 1-indexed\n",
    "\n",
    "# Apply color based on first occurrence and duplicates\n",
    "for row in range(2, len(df) + 2):  # Start from row 2 to skip the header\n",
    "    cell = ws.cell(row=row, column=eDateTime_col)\n",
    "    if df.at[row - 2, 'is_first_occurrence']:\n",
    "        cell.fill = green_fill\n",
    "    elif df.at[row - 2, 'is_duplicate']:\n",
    "        cell.fill = red_fill\n",
    "\n",
    "# Add a summary table for green and red counts\n",
    "summary_row = len(df) + 4  # Place it a few rows after the data\n",
    "ws.cell(row=summary_row, column=1, value=\"Color\")\n",
    "ws.cell(row=summary_row, column=2, value=\"Count\")\n",
    "ws.cell(row=summary_row + 1, column=1, value=\"Green (First Occurrences)\")\n",
    "ws.cell(row=summary_row + 1, column=2, value=green_count)\n",
    "ws.cell(row=summary_row + 2, column=1, value=\"Red (Subsequent Duplicates)\")\n",
    "ws.cell(row=summary_row + 2, column=2, value=red_count)\n",
    "\n",
    "# Create a pie chart for the green and red counts\n",
    "pie = PieChart()\n",
    "labels = Reference(ws, min_col=1, min_row=summary_row + 1, max_row=summary_row + 2)\n",
    "data = Reference(ws, min_col=2, min_row=summary_row, max_row=summary_row + 2)\n",
    "pie.add_data(data, titles_from_data=True)\n",
    "pie.set_categories(labels)\n",
    "pie.title = \"eDateTime Duplicate Analysis\"\n",
    "\n",
    "# Place the pie chart in the Excel sheet\n",
    "ws.add_chart(pie, f\"D{summary_row}\")\n",
    "\n",
    "# Save the updated workbook\n",
    "wb.save(\"highlighted_duplicates_with_summary_corrected.xlsx\")\n",
    "wb.close()\n",
    "\n",
    "print(\"Duplicates highlighted and summary saved to 'highlighted_duplicates_with_summary_corrected.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date):\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "\n",
    "\n",
    "    # Create a query string to fetch data for the given IMEIs within the specified date range\n",
    "\n",
    "    imei_str = ', '.join(f\"'{imei}'\" for imei in imei_list)  # Formatting IMEIs for SQL query\n",
    "\n",
    "    query = f\"\"\"\n",
    "\n",
    "        SELECT * \n",
    "\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "\n",
    "        WHERE deviceId IN ({imei_str})\n",
    "\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}';\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Run the query and get the result\n",
    "\n",
    "        query_job = client.query(query)\n",
    "\n",
    "        results = query_job.result()\n",
    "\n",
    "\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "\n",
    "        rows = [dict(row) for row in results]\n",
    "\n",
    "        return rows\n",
    "\n",
    "\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def save_to_excel(data, file_name):\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "\n",
    "    df.to_excel(file_name, index=False)\n",
    "\n",
    "    print(f\"Column names: {', '.join(df.columns)}\")\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "\n",
    "\n",
    "    # List of IMEIs to fetch data for\n",
    "\n",
    "    imei_list = ['352914090435716']\n",
    "\n",
    "    \n",
    "\n",
    "    # Define the date range\n",
    "\n",
    "    start_date = '2024-11-18'\n",
    "\n",
    "    end_date = '2024-11-20'\n",
    "\n",
    "\n",
    "\n",
    "    # Fetch data from BigQuery for specified IMEIs within the date range\n",
    "\n",
    "    data = read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date)\n",
    "\n",
    "\n",
    "\n",
    "    # Check if data was fetched successfully\n",
    "\n",
    "    if data:\n",
    "\n",
    "        # Define the output Excel file name\n",
    "\n",
    "        excel_file_name = \"bgqcan.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "        # Save the data to an Excel file\n",
    "\n",
    "        save_to_excel(data, excel_file_name)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"No data found.\")\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date):\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "\n",
    "\n",
    "    # Create a query string to fetch data for the given IMEIs within the specified date range\n",
    "\n",
    "    imei_str = ', '.join(f\"'{imei}'\" for imei in imei_list)  # Formatting IMEIs for SQL query\n",
    "\n",
    "    query = f\"\"\"\n",
    "\n",
    "        SELECT * \n",
    "\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "\n",
    "        WHERE deviceId IN ({imei_str})\n",
    "\n",
    "        AND DATE(eDate) BETWEEN '{start_date}' AND '{end_date}';\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Run the query and get the result\n",
    "\n",
    "        query_job = client.query(query)\n",
    "\n",
    "        results = query_job.result()\n",
    "\n",
    "\n",
    "\n",
    "        # Collect and return the rows as a list of dictionaries\n",
    "\n",
    "        rows = [dict(row) for row in results]\n",
    "\n",
    "        return rows\n",
    "\n",
    "\n",
    "\n",
    "    except GoogleAPIError as e:\n",
    "\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def save_to_excel(data, file_name):\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "\n",
    "    df.to_excel(file_name, index=False)\n",
    "\n",
    "    print(f\"Column names: {', '.join(df.columns)}\")\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Replace with your project_id, dataset_id, and table_id\n",
    "\n",
    "    project_id = \"conn-datalake-prod\"\n",
    "\n",
    "    dataset_id = \"bronze_telematics\"\n",
    "\n",
    "    table_id = \"canbs4_pkt\"  # Update with your actual table ID\n",
    "\n",
    "\n",
    "\n",
    "    # List of IMEIs to fetch data for\n",
    "\n",
    "    imei_list = ['352914090435716']\n",
    "\n",
    "    \n",
    "\n",
    "    # Define the date range\n",
    "\n",
    "    start_date = '2024-11-18'\n",
    "\n",
    "    end_date = '2024-11-20'\n",
    "\n",
    "\n",
    "\n",
    "    # Fetch data from BigQuery for specified IMEIs within the date range\n",
    "\n",
    "    data = read_data_from_bigquery(project_id, dataset_id, table_id, imei_list, start_date, end_date)\n",
    "\n",
    "\n",
    "\n",
    "    # Check if data was fetched successfully\n",
    "\n",
    "    if data:\n",
    "\n",
    "        # Define the output Excel file name\n",
    "\n",
    "        excel_file_name = \"bgqcan.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "        # Save the data to an Excel file\n",
    "\n",
    "        save_to_excel(data, excel_file_name)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
